{"cells":[{"cell_type":"markdown","id":"88c639dc-bfcd-449e-bddd-55f8bd551dd5","metadata":{},"source":["![Facial Recognition](facialrecognition.jpg)"]},{"cell_type":"markdown","id":"0fd04b96-d360-411c-8b17-a10382c97d29","metadata":{},"source":["You are a member of an elite group of data scientists, specialising in advanced facial recognition technology, this firm is dedicated to identifying and safeguarding prominent individuals from various spheresâ€”ranging from entertainment and sports to politics and philanthropy. The team's mission is to deploy AI-driven solutions that can accurately distinguish between images of notable personalities and the general populace, enhancing the personal security of such high-profile individuals. You're to focus on Arnold Schwarzenegger, a figure whose accomplishments span from bodybuilding champion to Hollywood icon, and from philanthropist to the Governor of California. "]},{"cell_type":"markdown","id":"be124832-8192-4f93-b487-247c3d03d23b","metadata":{},"source":["### **The Data**\n","The `data/lfw_arnie_nonarnie.csv` dataset contains processed facial image data derived from the \"Labeled Faces in the Wild\" (LFW) dataset, focusing specifically on images of Arnold Schwarzenegger and other individuals not identified as him. This dataset has been prepared to aid in the development and evaluation of facial recognition models. There are 40 images of Arnold Schwarzenegger and 150 of other people.\n","\n","| Column Name | Description |\n","|-------------|-------------|\n","| PC1, PC2, ... PCN | Principal components from PCA, capturing key image features. |\n","| Label | Binary indicator: `1` for Arnold Schwarzenegger, `0` for others. |"]},{"cell_type":"markdown","id":"64b99d2d","metadata":{},"source":["Leverage machine learning to enhance the security of influential figures by distinguishing Arnold Schwarzenegger from others.\n","\n","- Construct machine learning pipelines for three classification models. Store these initialized models in a dictionary named models.\n","- Determine the best performing model based on cross-validation scores. Save the model's name as best_model_name, its parameters as best_model_info, and its cross-validation score as best_model_cv_score.\n","- Evaluate the selected model and store accuracy, precision, recall, and f1 on the test set.\n","- Aim to achieve a minimum accuracy of 80% for at least one of the models. Save your best accuracy score as score.\n","\n","As an optional step, use Matplotlib to create a confusion matrix visualization for the predictions made by your best model."]},{"cell_type":"code","execution_count":null,"id":"2e0be28b-29dd-49bd-8e8b-f3cb828a5065","metadata":{},"outputs":[],"source":["# Import required libraries\n","import pandas as pd\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Read the CSV file \n","df = pd.read_csv(\"data/lfw_arnie_nonarnie.csv\")\n","\n","# Seperate the predictor and class label\n","X = df.drop('Label', axis=1)\n","y = df['Label'] \n","\n","# Split the data into training and testing sets using stratify to balance the class\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)"]},{"cell_type":"code","execution_count":null,"id":"f09a4e74-4a69-4dfc-8a54-f285653796d9","metadata":{},"outputs":[],"source":["# Start coding here"]},{"cell_type":"markdown","id":"a4b268ea","metadata":{},"source":["# Solution"]},{"cell_type":"code","execution_count":null,"id":"4603c5ea","metadata":{},"outputs":[],"source":["# Import required libraries\n","import pandas as pd\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Read the CSV file \n","df = pd.read_csv(\"data/lfw_arnie_nonarnie.csv\")\n","\n","# Seperate the predictor and class label\n","X = df.drop('Label', axis=1)\n","y = df['Label'] \n","\n","# Split the data into training and testing sets using stratify to balance the class\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)\n","\n","# Import the models you want to use\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Store initialized models in a dictionary\n","# This approach allows for easy expansion and comparison of different models\n","models = {\"LogisticRegression\": LogisticRegression(), \n","          \"KNeighborsClassifier\": KNeighborsClassifier(),\n","          \"DecisionTreeClassifier\": DecisionTreeClassifier()}\n","\n","# Store the model parameters in a dictionary\n","# Parameters are tailored to each model to explore a range of options during Grid Search\n","param_grid = {\"LogisticRegression\": {\"LogisticRegression__C\": [0.01, 0.1, 1, 10]},\n","              \"KNeighborsClassifier\": {\"KNeighborsClassifier__n_neighbors\": range(1,10)},\n","              \"DecisionTreeClassifier\": {\"DecisionTreeClassifier__max_depth\": [2, 5, 10],\n","           \"DecisionTreeClassifier__min_samples_split\": [2, 5, 10, 20],\n","           \"DecisionTreeClassifier__random_state\": [42]}}\n","\n","# Define cross-validation parameters\n","# KFold is used here to ensure that our model generalizes well on unseen data\n","kf = KFold(n_splits=5, random_state=42, shuffle=True)\n","\n","# Prepare to collect Grid Search CV results\n","# Grid Search helps find the best parameter combination for each model\n","pipe_accuracies = {}\n","pipe_params = {}\n","pipelines = {}\n","\n","# Create separate pipelines for each model, loop through the models and perform GridSearchCV\n","# Grid Search helps find the best parameter combination for each model\n","# Pipelines integrate preprocessing (e.g., scaling) with the model for cleaner code and to prevent data leakage\n","for name, model in models.items():\n","    pipeline = Pipeline(steps=[\n","        (\"scaler\", StandardScaler()),\n","        (name, model)\n","    ])\n","    # Create the GridSearchCV object\n","    grid_search = GridSearchCV(pipeline, param_grid[name], cv=kf, scoring=\"accuracy\")\n","    \n","    # Perform grid search and fit the model and store the results\n","    grid_search.fit(X_train, y_train)\n","    pipe_accuracies[name] = grid_search.best_score_\n","    pipe_params[name] = grid_search.best_params_\n","    pipelines[name] = grid_search\n","\n","# Select the best model based on the best cross-validation score\n","best_model_name = max(pipe_accuracies)\n","best_model_cv_score = max(pipe_accuracies.values())\n","best_model_info = pipe_params[best_model_name]\n","\n","# Print the best model name, parameters, and CV score\n","print(f\"Best Model: {best_model_name}\")\n","print(f\"Best Model Parameters: {best_model_info}\")\n","print(f\"Best Model CV Score: {best_model_cv_score}\")\n","\n","\n","# Compute and print key performance metrics\n","# These metrics help us understand the model's effectiveness in classification\n","y_pred = pipelines[best_model_name].predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1 Score: {f1:.4f}\")\n","\n","\n","# Optional: visualize the confusion matrix using Matplotlib\n","# This helps in understanding how well the model is predicting each class\n","conf_mat = confusion_matrix(y_test, y_pred)\n","plt.imshow(conf_mat, interpolation='nearest', cmap=plt.cm.Blues)\n","plt.title(f'Confusion Matrix for {best_model_name}')\n","plt.colorbar()\n","tick_marks = range(len(set(y_test)))\n","plt.xticks(tick_marks, ['Non-Arnie', 'Arnie'], rotation=45)\n","plt.yticks(tick_marks, ['Non-Arnie', 'Arnie'])\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
