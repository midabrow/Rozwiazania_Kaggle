{"cells":[{"cell_type":"markdown","id":"806ef75e-0551-4d96-912d-b44a6aa5ca20","metadata":{},"source":["![Crowded city](city-1265055_1280.jpg)\n","\n","In the quest for efficiency and effectiveness in urban transportation, finding the optimal routes to take passengers from their initial locations to their desired destinations is paramount. This challenge is not just about reducing travel time; it's about enhancing the overall experience for both drivers and passengers, ensuring safety, and minimizing environmental impact. \n","\n","You have been asked to revolutionize the way taxis navigate the urban landscape, ensuring passengers reach their destinations swiftly, safely, and satisfactorily. As an initial step, your goal is to build a reinforcement learning agent that solves this problem within a simulated environment.\n","\n","## The Taxi-v3 environment\n","The Taxi-v3 environment is a strategic simulation, offering a grid-based arena where a taxi navigates to address daily challenges akin to those faced by a taxi driver. This environment is defined by a 5x5 grid where the taxi's mission involves picking up a passenger from one of four specific locations (marked as Red, Green, Yellow, and Blue) and dropping them off at another designated spot. The goal is to accomplish this with minimal time on the road to maximize rewards, emphasizing the need for route optimization and efficient decision-making for passenger pickup and dropoff.\n","\n","### Key Components:\n","- **Action Space:** Comprises six actions where 0 moves the taxi south, 1 north, 2 east, 3 west, 4 picks up a passenger, and 5 drops off a passenger.\n","- **Observation Space:** Comprises 500 discrete states, accounting for 25 taxi positions, 5 potential passenger locations, and 4 destinations. \n","- **Rewards System:** Includes a penalty of -1 for each step taken without other rewards, +20 for successful passenger delivery, and -10 for illegal pickup or dropoff actions. Actions resulting in no operation, like hitting a wall, also incur a time step penalty.\n","\n","![Taxi-v3 environment snapshot](Taxi_snap.png)\n"]},{"cell_type":"code","execution_count":1,"id":"f59944f1-c11a-48e7-a1aa-10ede08e0ccf","metadata":{"collapsed":true,"executionCancelledAt":null,"executionTime":6919,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1708764694594,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Re-run this cell to install and import the necessary libraries and load the required variables\n!pip install gymnasium[toy_text] imageio\nimport numpy as np\nimport gymnasium as gym\nimport imageio\nfrom IPython.display import Image\nfrom gymnasium.utils import seeding\n\n# Initialize the Taxi-v3 environment\nenv = gym.make(\"Taxi-v3\", render_mode='rgb_array')\n\n# Seed the environment for reproducibility\nenv.np_random, _ = seeding.np_random(42)\nenv.action_space.seed(42)\nnp.random.seed(42)\n\n# Maximum number of actions per training episode\nmax_actions = 100 ","outputsMetadata":{"0":{"height":537,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (2.21.1)\n","Collecting gymnasium[toy_text]\n","  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[toy_text]) (1.23.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[toy_text]) (2.1.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[toy_text]) (4.9.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium[toy_text])\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[toy_text]) (6.8.0)\n","Collecting pygame>=2.1.3 (from gymnasium[toy_text])\n","  Downloading pygame-2.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.8/dist-packages (from imageio) (9.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium[toy_text]) (3.8.1)\n","Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Downloading pygame-2.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m121.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: farama-notifications, pygame, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 pygame-2.5.2\n"]}],"source":["# Re-run this cell to install and import the necessary libraries and load the required variables\n","!pip install gymnasium[toy_text] imageio\n","import numpy as np\n","import gymnasium as gym\n","import imageio\n","from IPython.display import Image\n","from gymnasium.utils import seeding\n","\n","# Initialize the Taxi-v3 environment\n","env = gym.make(\"Taxi-v3\", render_mode='rgb_array')\n","\n","# Seed the environment for reproducibility\n","env.np_random, _ = seeding.np_random(42)\n","env.action_space.seed(42)\n","np.random.seed(42)\n","\n","# Maximum number of actions per training episode\n","max_actions = 100 "]},{"cell_type":"markdown","id":"49bbabd0","metadata":{},"source":["- Train an agent over 2,000 episodes, allowing for a maximum of 100 actions per episode (max_actions), utilizing Q-learning. Record the total rewards achieved in each episode and save these in a list named episode_returns.\n","- What are the learned Q-values? Save these in a numpy array named q_table.\n","- What is the learned policy? Save it in a dictionary named policy.\n","- Test the agent's learned policy for one episode, starting with a seed of 42. Save the encountered states from env.render() as frames in a list named frames, and the sum of collected rewards in a variable named episode_total_reward. Make sure your agent does not execute more than 16 actions to solve the episode. If your learning process is efficient, the episode_total_reward should be at least 4.\n","- Execute the last provided cell to visualize your agent's performance in navigating the environment effectively. Please note that it might take up to one minute to render."]},{"cell_type":"code","execution_count":2,"id":"bbc86a7b-9bf9-4ec5-bc88-4098a70e392e","metadata":{"executionCancelledAt":null,"executionTime":2317,"lastExecutedAt":1708764703250,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Start coding here\n# Feel free to add as many cells as you want\n# Parameters for training\nepsilon = 1.0\nmin_epsilon = 0.01\nepsilon_decay = 0.001\nalpha = 0.1  # Learning rate\ngamma = 1 # Discount factor\n\n\n# Determine the environment's number of states and actions\nnum_states = env.observation_space.n\nnum_actions = env.action_space.n\n\n# Initialize the Q-table with zeros\nq_table = np.zeros((num_states, num_actions))\n\n# Epsilon-greedy strategy function\ndef epsilon_greedy(state):\n    if np.random.rand() < epsilon:\n        return env.action_space.sample()  # Explore\n    else:\n        return np.argmax(q_table[state])  # Exploit\n\n    \n# Q-learning update function\ndef q_learning_update(state, action, reward, next_state):\n    old_value = q_table[state, action]\n    next_max = max(q_table[next_state]) \n    q_table[state, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n\n# List to store the total reward per episode\nepisode_returns = []\n\n# Training loop\nfor episode in range(2000):\n    state, info = env.reset()\n    done = False\n    total_reward = 0\n\n    for i in range(max_actions):\n        action = epsilon_greedy(state)\n        next_state, reward, done, _, _ = env.step(action)\n        q_learning_update(state, action, reward, next_state)\n        state = next_state\n        total_reward += reward\n        if done:\n          break\n          \n    episode_returns.append(total_reward)\n    # Decay epsilon\n    epsilon = max(min_epsilon, epsilon*epsilon_decay)\n\n# Deriving the policy    \npolicy = {state: np.argmax(q_table[state]) for state in range(num_states)} \n\n## Testing the agent's behavior\n\n# List to store frames\nframes = []\nstate, info = env.reset(seed=42)\nframes.append(env.render())\nepisode_total_reward = 0\nfor i in range(16): # Execute maximum 16 moves\n    action = policy[state] \n    state, reward, done, _, _ = env.step(action)\n    episode_total_reward += reward\n    frames.append(env.render())\n    if done:\n      break      "},"outputs":[],"source":["# Start coding here\n","# Feel free to add as many cells as you want"]},{"cell_type":"code","execution_count":null,"id":"93ad05af-2e0f-4ada-b368-fabb0715bb30","metadata":{},"outputs":[],"source":["# Once you are done, run this cell to visualize the agent's behavior through the episode\n","# Save frames as a GIF\n","imageio.mimsave('taxi_agent_behavior.gif', frames, fps=5)\n","\n","# Display GIF\n","gif_path = \"taxi_agent_behavior.gif\" \n","Image(gif_path) "]},{"cell_type":"markdown","id":"2732cf7d","metadata":{},"source":["# Solution"]},{"cell_type":"code","execution_count":null,"id":"d222f7ef","metadata":{},"outputs":[],"source":["# Parameters for training\n","epsilon = 1.0\n","min_epsilon = 0.01\n","epsilon_decay = 0.001\n","alpha = 0.1  # Learning rate\n","gamma = 1 # Discount factor\n","\n","\n","# Determine the environment's number of states and actions\n","num_states = env.observation_space.n\n","num_actions = env.action_space.n\n","\n","# Initialize the Q-table with zeros\n","q_table = np.zeros((num_states, num_actions))\n","\n","# Epsilon-greedy strategy function\n","def epsilon_greedy(state):\n","    if np.random.rand() < epsilon:\n","        return env.action_space.sample()  # Explore\n","    else:\n","        return np.argmax(q_table[state])  # Exploit\n","\n","    \n","# Q-learning update function\n","def q_learning_update(state, action, reward, next_state):\n","    old_value = q_table[state, action]\n","    next_max = max(q_table[next_state]) \n","    q_table[state, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","\n","# List to store the total reward per episode\n","episode_returns = []\n","\n","# Training loop\n","for episode in range(2000):\n","    state, info = env.reset()\n","    terminated = False\n","    total_reward = 0\n","\n","    for i in range(max_actions):\n","        action = epsilon_greedy(state)\n","        next_state, reward, terminated, truncated, info = env.step(action)\n","        q_learning_update(state, action, reward, next_state)\n","        state = next_state\n","        total_reward += reward\n","        if terminated:\n","          break\n","          \n","    episode_returns.append(total_reward)\n","    # Decay epsilon\n","    epsilon = max(min_epsilon, epsilon*epsilon_decay)\n","\n","# Deriving the policy    \n","policy = {state: np.argmax(q_table[state]) for state in range(num_states)} \n","\n","## Testing the agent's behavior\n","\n","# List to store frames\n","frames = []\n","state, info = env.reset(seed=42)\n","frames.append(env.render())\n","episode_total_reward = 0\n","for i in range(16): # Execute maximum 16 moves\n","    action = policy[state] \n","    state, reward, terminated, truncated, info = env.step(action)\n","    episode_total_reward += reward\n","    frames.append(env.render())\n","    if terminated:\n","      break      "]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
