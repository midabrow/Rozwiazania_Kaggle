{"cells":[{"cell_type":"markdown","id":"b1309988-b429-4fb0-8c4c-193582dbec93","metadata":{},"source":["![mobydick](mobydick.jpg)"]},{"cell_type":"markdown","id":"611e416c-70e7-478a-a3c8-e54f3fdb4a7f","metadata":{},"source":["In this workspace, you'll scrape the novel Moby Dick from the website [Project Gutenberg](https://www.gutenberg.org/) (which contains a large corpus of books) using the Python `requests` package. You'll extract words from this web data using `BeautifulSoup` before analyzing the distribution of words using the Natural Language ToolKit (`nltk`) and `Counter`.\n","\n","The Data Science pipeline you'll build in this workspace can be used to visualize the word frequency distributions of any novel you can find on Project Gutenberg."]},{"cell_type":"markdown","id":"55926791","metadata":{},"source":["What are the most frequent words in Herman Melville's novel Moby Dick, and how often do they occur?\n","\n","Note that the HTML file you are asked to request is a cashed version of this file from Project Gutenberg.\n","\n","Your project will follow these steps:\n","\n","- The first step will be to request the Moby Dick HTML file using requests and encoding it to utf-8. Here is the URL to scrape from: https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\n","- Next, you'll extract the HTML and create a BeautifulSoup object using an HTML parser to get the text.\n","- Following that, you'll initialize a regex tokenizer object tokenizer using nltk.tokenize.RegexpTokenizer to keep only alphanumeric text, assigning the results to tokens.\n","- You'll transform the tokens into lowercase, removing English stop words, and saving the results to words_no_stop.\n","- Finally, you'll initialize a Counter object and find the ten most common words, saving the result to top_ten and printing to see what they are."]},{"cell_type":"code","execution_count":2,"id":"15b5f52f-fd9b-4f0e-9fcc-f7733022c7c0","metadata":{"collapsed":false,"executionTime":35,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastSuccessfullyExecutedCode":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Start coding here... "},"outputs":[],"source":["# Import and download packages\n","import requests\n","from bs4 import BeautifulSoup\n","import nltk\n","from collections import Counter\n","nltk.download('stopwords')\n","\n","# Start coding here... "]},{"cell_type":"markdown","id":"dd391e09","metadata":{},"source":["# Solution"]},{"cell_type":"code","execution_count":null,"id":"ae90185e","metadata":{},"outputs":[],"source":["# Import and download packages\n","import requests\n","from bs4 import BeautifulSoup\n","import nltk\n","from collections import Counter\n","nltk.download('stopwords')\n","\n","# Get the Moby Dick HTML  \n","r = requests.get('https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm')\n","\n","# Set the correct text encoding of the HTML page\n","r.encoding = 'utf-8'\n","\n","# Extract the HTML from the request object\n","html = r.text\n","\n","# Print the first 2000 characters in html\n","print(html[0:2000])\n","\n","# Create a BeautifulSoup object from the HTML\n","html_soup = BeautifulSoup(html, \"html.parser\")\n","\n","# Get the text out of the soup\n","moby_text = html_soup.get_text()\n","\n","# Create a tokenizer\n","tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n","\n","# Tokenize the text\n","tokens = tokenizer.tokenize(moby_text)\n","\n","# Create a list called words containing all tokens transformed to lowercase\n","words = [token.lower() for token in tokens]\n","\n","# Print out the first eight words\n","words[:8]\n","\n","# Get the English stop words from nltk\n","stop_words = nltk.corpus.stopwords.words('english')\n","\n","# Print out the first eight stop words\n","stop_words[:8]\n","\n","# Create a list words_ns containing all words that are in words but not in stop_words\n","words_no_stop = [word for word in words if word not in stop_words]\n","\n","# Print the first five words_no_stop to check that stop words are gone\n","words_no_stop[:5]\n","\n","# Initialize a Counter object from our processed list of words\n","count = Counter(words_no_stop)\n","\n","# Store ten most common words and their counts as top_ten\n","top_ten = count.most_common(10)\n","\n","# Print the top ten words and their counts\n","print(top_ten)"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
